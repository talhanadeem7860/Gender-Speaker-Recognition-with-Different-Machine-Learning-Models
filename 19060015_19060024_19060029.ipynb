{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Phase 3 - Gender & Speaker Recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import python_speech_features as mfcc\n",
    "from scipy.io.wavfile import read\n",
    "import numpy as np\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Functions to get feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_MFCC(audio, sr):\n",
    "    features = mfcc.mfcc(audio, sr, 0.025, 0.01, 13, appendEnergy = True)\n",
    "    return np.mean(features, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(data):\n",
    "    x, m = np.shape(data)\n",
    "    In = []\n",
    "    Out = []\n",
    "    for i in range (x):\n",
    "        In = data[i]\n",
    "        feature = get_MFCC(In[1], In[0])\n",
    "        Out.append(feature) \n",
    "    return Out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made 4 np arrays:\n",
    "- gtrain_X & gtrain_Y\n",
    "- gtest_X & gtest_Y\n",
    "\n",
    "X data sets have 13 features in reach row for each input .wav file\n",
    "\n",
    "Y data has corresponding gender (2 classes - 0 and 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "suffix = \"_F\";\n",
    "train = []\n",
    "gtrain_Y = []\n",
    "train_list = glob.glob(os.path.join(os.getcwd(), \"Gender_Recognition/Train\"))\n",
    "for train_path in train_list:\n",
    "    for subdir, dirs, files in os.walk(train_path):\n",
    "        for file in files:\n",
    "            filepath = subdir + os.sep + file\n",
    "            if os.path.dirname(filepath).endswith(suffix):\n",
    "                temp = 1;\n",
    "            else:\n",
    "                temp = 0;\n",
    "            if filepath.endswith(\".wav\"):\n",
    "                with open(filepath) as train_input:\n",
    "                    train.append(read(filepath))\n",
    "                    gtrain_Y.append(temp)\n",
    "gtrain_Y = list(map(int, gtrain_Y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "gtest_Y = []\n",
    "test_list = glob.glob(os.path.join(os.getcwd(), \"Gender_Recognition/Test\"))\n",
    "for test_path in test_list:\n",
    "    for subdir, dirs, files in os.walk(test_path):\n",
    "        for file in files:\n",
    "            filepath = subdir + os.sep + file\n",
    "            if os.path.dirname(filepath).endswith(suffix):\n",
    "                temp = 1;\n",
    "            else:\n",
    "                temp = 0;\n",
    "            if filepath.endswith(\".wav\"):\n",
    "                with open(filepath) as test_input:\n",
    "                    test.append(read(filepath))\n",
    "                    gtest_Y.append(temp)\n",
    "gtest_Y = list(map(int, gtest_Y)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\talha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\numpy\\core\\_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n"
     ]
    }
   ],
   "source": [
    "gtrain_X = get_features(train)\n",
    "gtest_X = get_features(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Speaker Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Made 4 np arrays:\n",
    "- strain_X & strain_Y\n",
    "- stest_X & stest_Y\n",
    "\n",
    "X data sets have 13 features in reach row for each input .wav file\n",
    "\n",
    "Y data has corresponding speaker (142 classes - 0 to 141)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = []\n",
    "trainY = []\n",
    "train_list = glob.glob(os.path.join(os.getcwd(), \"Speaker_Recognition/Train\"))\n",
    "for train_path in train_list:\n",
    "    for subdir, dirs, files in os.walk(train_path):\n",
    "        for file in files:\n",
    "            filepath = subdir + os.sep + file\n",
    "            direc = os.path.dirname(filepath)\n",
    "            direc = direc[-5:]\n",
    "            temp = direc[:-2]\n",
    "            if filepath.endswith(\".wav\"):\n",
    "                with open(filepath) as train_input:\n",
    "                    train.append(read(filepath))\n",
    "                    trainY.append(temp)\n",
    "trainY = list(map(int, trainY)) \n",
    "strain_Y = np.array(trainY) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "testY = []\n",
    "test_list = glob.glob(os.path.join(os.getcwd(), \"Speaker_Recognition/Test\"))\n",
    "for test_path in test_list:\n",
    "    for subdir, dirs, files in os.walk(test_path):\n",
    "        for file in files:\n",
    "            filepath = subdir + os.sep + file\n",
    "            direc = os.path.dirname(filepath)\n",
    "            direc = direc[-5:]\n",
    "            temp = direc[:-2]\n",
    "            if filepath.endswith(\".wav\"):\n",
    "                with open(filepath) as test_input:\n",
    "                    test.append(read(filepath))\n",
    "                    testY.append(temp)\n",
    "testY = list(map(int, testY)) \n",
    "stest_Y = np.array(testY) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "strain_X = get_features(train)\n",
    "stest_X = get_features(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting sklearn implementation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation(test_Y, result):\n",
    "    print('Accuracy is: ', accuracy_score(result, test_Y)*100,'%')\n",
    "    print('Confusion Matrix is:') \n",
    "    print(confusion_matrix(result, test_Y))\n",
    "    print(classification_report(test_Y, result))\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import f1_score\n",
    "mlp = MLPClassifier(max_iter = 5000, activation = 'logistic', solver = 'sgd', random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "paramgrid = {'hidden_layer_sizes': [(128,64), (64), (64,32), (32)],\n",
    "             'learning_rate_init': [0.4, 0.1, 0.01]}\n",
    "gcs = GridSearchCV(mlp, paramgrid, scoring = 'f1_macro', cv = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender Recognition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_res = gcs.fit(gtrain_X, gtrain_Y)\n",
    "best_params_g = grid_res.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are:\n",
      "{'hidden_layer_sizes': 32, 'learning_rate_init': 0.01}\n"
     ]
    }
   ],
   "source": [
    "print('The best parameters are:')\n",
    "print(best_params_g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  87.05882352941177 %\n",
      "Confusion Matrix is:\n",
      "[[121  13]\n",
      " [  9  27]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.93      0.92       130\n",
      "           1       0.75      0.68      0.71        40\n",
      "\n",
      "    accuracy                           0.87       170\n",
      "   macro avg       0.83      0.80      0.81       170\n",
      "weighted avg       0.87      0.87      0.87       170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = grid_res.predict(gtest_X)\n",
    "evaluation(gtest_Y, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speaker Recognition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_res_s = gcs.fit(strain_X, strain_Y)\n",
    "best_params_s = grid_res_s.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best parameters are:\n",
      "{'hidden_layer_sizes': 64, 'learning_rate_init': 0.01}\n"
     ]
    }
   ],
   "source": [
    "print('The best parameters are:')\n",
    "print(best_params_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  94.36619718309859 %\n",
      "Confusion Matrix is:\n",
      "[[2 0 0 ... 0 0 0]\n",
      " [0 2 0 ... 0 0 0]\n",
      " [0 0 2 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 2 0 0]\n",
      " [0 0 0 ... 0 2 0]\n",
      " [0 0 0 ... 0 0 2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       0.67      1.00      0.80         2\n",
      "           6       1.00      1.00      1.00         2\n",
      "           7       1.00      1.00      1.00         2\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       1.00      1.00      1.00         2\n",
      "          10       1.00      1.00      1.00         2\n",
      "          11       1.00      1.00      1.00         2\n",
      "          12       1.00      1.00      1.00         2\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       1.00      1.00      1.00         2\n",
      "          15       1.00      1.00      1.00         2\n",
      "          16       1.00      1.00      1.00         2\n",
      "          17       1.00      1.00      1.00         2\n",
      "          18       1.00      1.00      1.00         2\n",
      "          19       1.00      0.50      0.67         2\n",
      "          20       0.67      1.00      0.80         2\n",
      "          21       1.00      1.00      1.00         2\n",
      "          22       0.67      1.00      0.80         2\n",
      "          23       1.00      1.00      1.00         2\n",
      "          24       1.00      1.00      1.00         2\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       1.00      1.00      1.00         2\n",
      "          27       1.00      0.50      0.67         2\n",
      "          28       1.00      1.00      1.00         2\n",
      "          29       0.50      0.50      0.50         2\n",
      "          30       1.00      1.00      1.00         2\n",
      "          31       1.00      1.00      1.00         2\n",
      "          32       0.67      1.00      0.80         2\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      0.50      0.67         2\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         2\n",
      "          37       1.00      1.00      1.00         2\n",
      "          38       1.00      0.50      0.67         2\n",
      "          39       1.00      1.00      1.00         2\n",
      "          40       1.00      1.00      1.00         2\n",
      "          41       1.00      0.50      0.67         2\n",
      "          42       1.00      1.00      1.00         2\n",
      "          43       1.00      1.00      1.00         2\n",
      "          44       1.00      0.50      0.67         2\n",
      "          45       1.00      1.00      1.00         2\n",
      "          46       1.00      1.00      1.00         2\n",
      "          47       1.00      1.00      1.00         2\n",
      "          48       1.00      0.50      0.67         2\n",
      "          49       1.00      1.00      1.00         2\n",
      "          50       1.00      1.00      1.00         2\n",
      "          51       1.00      1.00      1.00         2\n",
      "          52       1.00      1.00      1.00         2\n",
      "          53       1.00      1.00      1.00         2\n",
      "          54       1.00      1.00      1.00         2\n",
      "          55       1.00      1.00      1.00         2\n",
      "          56       1.00      1.00      1.00         2\n",
      "          57       1.00      1.00      1.00         2\n",
      "          58       0.67      1.00      0.80         2\n",
      "          59       1.00      1.00      1.00         2\n",
      "          60       1.00      1.00      1.00         2\n",
      "          61       1.00      1.00      1.00         2\n",
      "          62       1.00      0.50      0.67         2\n",
      "          63       0.67      1.00      0.80         2\n",
      "          64       0.67      1.00      0.80         2\n",
      "          65       1.00      1.00      1.00         2\n",
      "          66       1.00      1.00      1.00         2\n",
      "          67       1.00      1.00      1.00         2\n",
      "          68       1.00      1.00      1.00         2\n",
      "          69       1.00      1.00      1.00         2\n",
      "          70       0.67      1.00      0.80         2\n",
      "          71       1.00      1.00      1.00         2\n",
      "          72       1.00      1.00      1.00         2\n",
      "          73       1.00      1.00      1.00         2\n",
      "          74       1.00      1.00      1.00         2\n",
      "          75       1.00      1.00      1.00         2\n",
      "          76       1.00      1.00      1.00         2\n",
      "          77       1.00      1.00      1.00         2\n",
      "          78       1.00      1.00      1.00         2\n",
      "          79       1.00      1.00      1.00         2\n",
      "          80       1.00      1.00      1.00         2\n",
      "          81       1.00      1.00      1.00         2\n",
      "          82       1.00      1.00      1.00         2\n",
      "          83       1.00      1.00      1.00         2\n",
      "          84       1.00      0.50      0.67         2\n",
      "          85       1.00      1.00      1.00         2\n",
      "          86       0.67      1.00      0.80         2\n",
      "          87       1.00      1.00      1.00         2\n",
      "          88       1.00      1.00      1.00         2\n",
      "          89       1.00      0.50      0.67         2\n",
      "          90       1.00      0.50      0.67         2\n",
      "          91       1.00      1.00      1.00         2\n",
      "          92       1.00      1.00      1.00         2\n",
      "          93       1.00      1.00      1.00         2\n",
      "          94       1.00      1.00      1.00         2\n",
      "          95       0.33      0.50      0.40         2\n",
      "          96       1.00      1.00      1.00         2\n",
      "          97       1.00      1.00      1.00         2\n",
      "          98       1.00      1.00      1.00         2\n",
      "          99       1.00      1.00      1.00         2\n",
      "         100       1.00      0.50      0.67         2\n",
      "         101       1.00      1.00      1.00         2\n",
      "         102       1.00      1.00      1.00         2\n",
      "         103       1.00      1.00      1.00         2\n",
      "         104       1.00      1.00      1.00         2\n",
      "         105       1.00      1.00      1.00         2\n",
      "         106       1.00      1.00      1.00         2\n",
      "         107       1.00      1.00      1.00         2\n",
      "         108       1.00      1.00      1.00         2\n",
      "         109       1.00      1.00      1.00         2\n",
      "         110       0.67      1.00      0.80         2\n",
      "         111       1.00      1.00      1.00         2\n",
      "         112       1.00      1.00      1.00         2\n",
      "         113       1.00      1.00      1.00         2\n",
      "         114       1.00      1.00      1.00         2\n",
      "         115       0.67      1.00      0.80         2\n",
      "         116       1.00      1.00      1.00         2\n",
      "         117       1.00      1.00      1.00         2\n",
      "         118       0.00      0.00      0.00         2\n",
      "         119       1.00      1.00      1.00         2\n",
      "         120       1.00      1.00      1.00         2\n",
      "         121       1.00      1.00      1.00         2\n",
      "         122       1.00      1.00      1.00         2\n",
      "         123       1.00      1.00      1.00         2\n",
      "         124       1.00      1.00      1.00         2\n",
      "         125       1.00      1.00      1.00         2\n",
      "         126       1.00      1.00      1.00         2\n",
      "         127       1.00      1.00      1.00         2\n",
      "         128       1.00      1.00      1.00         2\n",
      "         129       1.00      1.00      1.00         2\n",
      "         130       1.00      1.00      1.00         2\n",
      "         131       1.00      1.00      1.00         2\n",
      "         132       1.00      1.00      1.00         2\n",
      "         133       1.00      1.00      1.00         2\n",
      "         134       1.00      1.00      1.00         2\n",
      "         135       1.00      1.00      1.00         2\n",
      "         136       0.67      1.00      0.80         2\n",
      "         137       1.00      1.00      1.00         2\n",
      "         138       1.00      1.00      1.00         2\n",
      "         139       1.00      1.00      1.00         2\n",
      "         140       1.00      1.00      1.00         2\n",
      "         141       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.94       284\n",
      "   macro avg       0.96      0.94      0.94       284\n",
      "weighted avg       0.96      0.94      0.94       284\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result = grid_res_s.predict(stest_X)\n",
    "evaluation(stest_Y, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "svc = LinearSVC(max_iter = 1e5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender Recognition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  83.52941176470588 %\n",
      "Confusion Matrix is:\n",
      "[[120  18]\n",
      " [ 10  22]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.92      0.90       130\n",
      "           1       0.69      0.55      0.61        40\n",
      "\n",
      "    accuracy                           0.84       170\n",
      "   macro avg       0.78      0.74      0.75       170\n",
      "weighted avg       0.83      0.84      0.83       170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gend = svc.fit(gtrain_X, gtrain_Y)\n",
    "result = gend.predict(gtest_X)\n",
    "evaluation(gtest_Y, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speaker Recognition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  84.50704225352112 %\n",
      "Confusion Matrix is:\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 2 0 ... 0 0 0]\n",
      " [0 0 2 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 2 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       0.00      0.00      0.00         2\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       0.67      1.00      0.80         2\n",
      "           6       0.33      1.00      0.50         2\n",
      "           7       0.50      0.50      0.50         2\n",
      "           8       0.50      1.00      0.67         2\n",
      "           9       1.00      1.00      1.00         2\n",
      "          10       1.00      1.00      1.00         2\n",
      "          11       1.00      0.50      0.67         2\n",
      "          12       0.67      1.00      0.80         2\n",
      "          13       0.67      1.00      0.80         2\n",
      "          14       1.00      1.00      1.00         2\n",
      "          15       1.00      1.00      1.00         2\n",
      "          16       0.67      1.00      0.80         2\n",
      "          17       1.00      1.00      1.00         2\n",
      "          18       0.67      1.00      0.80         2\n",
      "          19       1.00      0.50      0.67         2\n",
      "          20       0.67      1.00      0.80         2\n",
      "          21       1.00      1.00      1.00         2\n",
      "          22       0.00      0.00      0.00         2\n",
      "          23       1.00      1.00      1.00         2\n",
      "          24       0.50      1.00      0.67         2\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       1.00      1.00      1.00         2\n",
      "          27       0.50      0.50      0.50         2\n",
      "          28       1.00      1.00      1.00         2\n",
      "          29       1.00      1.00      1.00         2\n",
      "          30       1.00      0.50      0.67         2\n",
      "          31       0.67      1.00      0.80         2\n",
      "          32       1.00      1.00      1.00         2\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       0.50      0.50      0.50         2\n",
      "          35       1.00      1.00      1.00         2\n",
      "          36       1.00      1.00      1.00         2\n",
      "          37       0.33      1.00      0.50         2\n",
      "          38       1.00      0.50      0.67         2\n",
      "          39       1.00      0.50      0.67         2\n",
      "          40       1.00      0.50      0.67         2\n",
      "          41       1.00      1.00      1.00         2\n",
      "          42       1.00      1.00      1.00         2\n",
      "          43       1.00      1.00      1.00         2\n",
      "          44       1.00      1.00      1.00         2\n",
      "          45       1.00      1.00      1.00         2\n",
      "          46       0.33      0.50      0.40         2\n",
      "          47       1.00      0.50      0.67         2\n",
      "          48       1.00      0.50      0.67         2\n",
      "          49       1.00      0.50      0.67         2\n",
      "          50       1.00      1.00      1.00         2\n",
      "          51       1.00      1.00      1.00         2\n",
      "          52       1.00      1.00      1.00         2\n",
      "          53       1.00      1.00      1.00         2\n",
      "          54       1.00      1.00      1.00         2\n",
      "          55       1.00      1.00      1.00         2\n",
      "          56       1.00      1.00      1.00         2\n",
      "          57       1.00      1.00      1.00         2\n",
      "          58       1.00      1.00      1.00         2\n",
      "          59       0.50      1.00      0.67         2\n",
      "          60       1.00      1.00      1.00         2\n",
      "          61       1.00      0.50      0.67         2\n",
      "          62       1.00      0.50      0.67         2\n",
      "          63       0.33      0.50      0.40         2\n",
      "          64       0.50      0.50      0.50         2\n",
      "          65       1.00      1.00      1.00         2\n",
      "          66       1.00      0.50      0.67         2\n",
      "          67       1.00      0.50      0.67         2\n",
      "          68       1.00      1.00      1.00         2\n",
      "          69       1.00      1.00      1.00         2\n",
      "          70       0.67      1.00      0.80         2\n",
      "          71       1.00      1.00      1.00         2\n",
      "          72       1.00      1.00      1.00         2\n",
      "          73       0.67      1.00      0.80         2\n",
      "          74       1.00      1.00      1.00         2\n",
      "          75       0.67      1.00      0.80         2\n",
      "          76       1.00      0.50      0.67         2\n",
      "          77       1.00      1.00      1.00         2\n",
      "          78       1.00      1.00      1.00         2\n",
      "          79       1.00      0.50      0.67         2\n",
      "          80       1.00      0.50      0.67         2\n",
      "          81       1.00      1.00      1.00         2\n",
      "          82       1.00      1.00      1.00         2\n",
      "          83       1.00      1.00      1.00         2\n",
      "          84       0.50      0.50      0.50         2\n",
      "          85       1.00      1.00      1.00         2\n",
      "          86       0.67      1.00      0.80         2\n",
      "          87       1.00      1.00      1.00         2\n",
      "          88       1.00      1.00      1.00         2\n",
      "          89       0.00      0.00      0.00         2\n",
      "          90       1.00      0.50      0.67         2\n",
      "          91       1.00      1.00      1.00         2\n",
      "          92       1.00      1.00      1.00         2\n",
      "          93       1.00      1.00      1.00         2\n",
      "          94       1.00      1.00      1.00         2\n",
      "          95       1.00      0.50      0.67         2\n",
      "          96       0.67      1.00      0.80         2\n",
      "          97       1.00      1.00      1.00         2\n",
      "          98       1.00      0.50      0.67         2\n",
      "          99       1.00      1.00      1.00         2\n",
      "         100       0.00      0.00      0.00         2\n",
      "         101       1.00      0.50      0.67         2\n",
      "         102       0.33      0.50      0.40         2\n",
      "         103       1.00      0.50      0.67         2\n",
      "         104       1.00      0.50      0.67         2\n",
      "         105       1.00      1.00      1.00         2\n",
      "         106       1.00      1.00      1.00         2\n",
      "         107       1.00      1.00      1.00         2\n",
      "         108       0.67      1.00      0.80         2\n",
      "         109       1.00      1.00      1.00         2\n",
      "         110       0.67      1.00      0.80         2\n",
      "         111       1.00      1.00      1.00         2\n",
      "         112       1.00      1.00      1.00         2\n",
      "         113       1.00      1.00      1.00         2\n",
      "         114       1.00      1.00      1.00         2\n",
      "         115       1.00      1.00      1.00         2\n",
      "         116       1.00      1.00      1.00         2\n",
      "         117       1.00      1.00      1.00         2\n",
      "         118       1.00      0.50      0.67         2\n",
      "         119       1.00      1.00      1.00         2\n",
      "         120       1.00      0.50      0.67         2\n",
      "         121       1.00      1.00      1.00         2\n",
      "         122       1.00      1.00      1.00         2\n",
      "         123       1.00      1.00      1.00         2\n",
      "         124       1.00      1.00      1.00         2\n",
      "         125       1.00      1.00      1.00         2\n",
      "         126       1.00      1.00      1.00         2\n",
      "         127       1.00      1.00      1.00         2\n",
      "         128       1.00      0.50      0.67         2\n",
      "         129       1.00      1.00      1.00         2\n",
      "         130       0.67      1.00      0.80         2\n",
      "         131       1.00      1.00      1.00         2\n",
      "         132       1.00      0.50      0.67         2\n",
      "         133       1.00      1.00      1.00         2\n",
      "         134       0.50      1.00      0.67         2\n",
      "         135       1.00      1.00      1.00         2\n",
      "         136       0.67      1.00      0.80         2\n",
      "         137       1.00      1.00      1.00         2\n",
      "         138       1.00      1.00      1.00         2\n",
      "         139       1.00      1.00      1.00         2\n",
      "         140       1.00      0.50      0.67         2\n",
      "         141       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.85       284\n",
      "   macro avg       0.88      0.85      0.84       284\n",
      "weighted avg       0.88      0.85      0.84       284\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\talha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "speak = svc.fit(strain_X, strain_Y)\n",
    "result = speak.predict(stest_X)\n",
    "evaluation(stest_Y, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "GNB = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gender Recognition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  85.29411764705883 %\n",
      "Confusion Matrix is:\n",
      "[[120  15]\n",
      " [ 10  25]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.92      0.91       130\n",
      "           1       0.71      0.62      0.67        40\n",
      "\n",
      "    accuracy                           0.85       170\n",
      "   macro avg       0.80      0.77      0.79       170\n",
      "weighted avg       0.85      0.85      0.85       170\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GNB.fit(gtrain_X, gtrain_Y)\n",
    "result = GNB.predict(gtest_X)\n",
    "evaluation(gtest_Y, result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Speaker Recognition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is:  92.25352112676056 %\n",
      "Confusion Matrix is:\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 2 0 ... 0 0 0]\n",
      " [0 0 2 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 2 0 0]\n",
      " [0 0 0 ... 0 2 0]\n",
      " [0 0 0 ... 0 0 2]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.50      0.67         2\n",
      "           1       1.00      1.00      1.00         2\n",
      "           2       1.00      1.00      1.00         2\n",
      "           3       1.00      1.00      1.00         2\n",
      "           4       1.00      1.00      1.00         2\n",
      "           5       1.00      1.00      1.00         2\n",
      "           6       1.00      1.00      1.00         2\n",
      "           7       1.00      1.00      1.00         2\n",
      "           8       1.00      1.00      1.00         2\n",
      "           9       1.00      1.00      1.00         2\n",
      "          10       1.00      1.00      1.00         2\n",
      "          11       1.00      1.00      1.00         2\n",
      "          12       1.00      1.00      1.00         2\n",
      "          13       1.00      1.00      1.00         2\n",
      "          14       1.00      1.00      1.00         2\n",
      "          15       1.00      1.00      1.00         2\n",
      "          16       0.40      1.00      0.57         2\n",
      "          17       1.00      1.00      1.00         2\n",
      "          18       1.00      1.00      1.00         2\n",
      "          19       1.00      0.50      0.67         2\n",
      "          20       0.67      1.00      0.80         2\n",
      "          21       1.00      1.00      1.00         2\n",
      "          22       1.00      0.50      0.67         2\n",
      "          23       1.00      1.00      1.00         2\n",
      "          24       1.00      1.00      1.00         2\n",
      "          25       1.00      1.00      1.00         2\n",
      "          26       1.00      1.00      1.00         2\n",
      "          27       1.00      1.00      1.00         2\n",
      "          28       1.00      1.00      1.00         2\n",
      "          29       1.00      1.00      1.00         2\n",
      "          30       0.50      1.00      0.67         2\n",
      "          31       1.00      1.00      1.00         2\n",
      "          32       1.00      1.00      1.00         2\n",
      "          33       1.00      1.00      1.00         2\n",
      "          34       1.00      0.50      0.67         2\n",
      "          35       0.50      1.00      0.67         2\n",
      "          36       1.00      1.00      1.00         2\n",
      "          37       1.00      1.00      1.00         2\n",
      "          38       1.00      0.50      0.67         2\n",
      "          39       1.00      1.00      1.00         2\n",
      "          40       1.00      0.50      0.67         2\n",
      "          41       1.00      0.50      0.67         2\n",
      "          42       1.00      1.00      1.00         2\n",
      "          43       1.00      1.00      1.00         2\n",
      "          44       1.00      0.50      0.67         2\n",
      "          45       1.00      1.00      1.00         2\n",
      "          46       1.00      1.00      1.00         2\n",
      "          47       0.50      1.00      0.67         2\n",
      "          48       0.67      1.00      0.80         2\n",
      "          49       1.00      1.00      1.00         2\n",
      "          50       1.00      1.00      1.00         2\n",
      "          51       1.00      1.00      1.00         2\n",
      "          52       1.00      1.00      1.00         2\n",
      "          53       1.00      1.00      1.00         2\n",
      "          54       1.00      0.50      0.67         2\n",
      "          55       1.00      1.00      1.00         2\n",
      "          56       1.00      1.00      1.00         2\n",
      "          57       1.00      1.00      1.00         2\n",
      "          58       0.50      1.00      0.67         2\n",
      "          59       1.00      1.00      1.00         2\n",
      "          60       1.00      1.00      1.00         2\n",
      "          61       1.00      1.00      1.00         2\n",
      "          62       1.00      1.00      1.00         2\n",
      "          63       0.67      1.00      0.80         2\n",
      "          64       1.00      1.00      1.00         2\n",
      "          65       1.00      1.00      1.00         2\n",
      "          66       1.00      0.50      0.67         2\n",
      "          67       1.00      1.00      1.00         2\n",
      "          68       0.67      1.00      0.80         2\n",
      "          69       1.00      1.00      1.00         2\n",
      "          70       1.00      0.50      0.67         2\n",
      "          71       1.00      1.00      1.00         2\n",
      "          72       1.00      1.00      1.00         2\n",
      "          73       1.00      1.00      1.00         2\n",
      "          74       1.00      1.00      1.00         2\n",
      "          75       1.00      1.00      1.00         2\n",
      "          76       1.00      1.00      1.00         2\n",
      "          77       1.00      1.00      1.00         2\n",
      "          78       1.00      1.00      1.00         2\n",
      "          79       1.00      1.00      1.00         2\n",
      "          80       0.50      1.00      0.67         2\n",
      "          81       1.00      1.00      1.00         2\n",
      "          82       1.00      1.00      1.00         2\n",
      "          83       1.00      1.00      1.00         2\n",
      "          84       0.33      0.50      0.40         2\n",
      "          85       0.67      1.00      0.80         2\n",
      "          86       1.00      0.50      0.67         2\n",
      "          87       1.00      1.00      1.00         2\n",
      "          88       1.00      1.00      1.00         2\n",
      "          89       1.00      0.50      0.67         2\n",
      "          90       1.00      1.00      1.00         2\n",
      "          91       1.00      1.00      1.00         2\n",
      "          92       1.00      1.00      1.00         2\n",
      "          93       1.00      1.00      1.00         2\n",
      "          94       1.00      1.00      1.00         2\n",
      "          95       0.50      0.50      0.50         2\n",
      "          96       1.00      1.00      1.00         2\n",
      "          97       1.00      1.00      1.00         2\n",
      "          98       1.00      0.50      0.67         2\n",
      "          99       1.00      1.00      1.00         2\n",
      "         100       0.00      0.00      0.00         2\n",
      "         101       1.00      1.00      1.00         2\n",
      "         102       1.00      1.00      1.00         2\n",
      "         103       1.00      1.00      1.00         2\n",
      "         104       1.00      0.50      0.67         2\n",
      "         105       1.00      1.00      1.00         2\n",
      "         106       1.00      1.00      1.00         2\n",
      "         107       1.00      1.00      1.00         2\n",
      "         108       1.00      1.00      1.00         2\n",
      "         109       1.00      1.00      1.00         2\n",
      "         110       1.00      1.00      1.00         2\n",
      "         111       1.00      1.00      1.00         2\n",
      "         112       0.67      1.00      0.80         2\n",
      "         113       1.00      1.00      1.00         2\n",
      "         114       1.00      1.00      1.00         2\n",
      "         115       1.00      1.00      1.00         2\n",
      "         116       1.00      1.00      1.00         2\n",
      "         117       1.00      1.00      1.00         2\n",
      "         118       0.00      0.00      0.00         2\n",
      "         119       1.00      1.00      1.00         2\n",
      "         120       1.00      1.00      1.00         2\n",
      "         121       1.00      1.00      1.00         2\n",
      "         122       1.00      1.00      1.00         2\n",
      "         123       1.00      0.50      0.67         2\n",
      "         124       1.00      1.00      1.00         2\n",
      "         125       1.00      1.00      1.00         2\n",
      "         126       1.00      1.00      1.00         2\n",
      "         127       1.00      1.00      1.00         2\n",
      "         128       1.00      1.00      1.00         2\n",
      "         129       1.00      1.00      1.00         2\n",
      "         130       1.00      1.00      1.00         2\n",
      "         131       1.00      1.00      1.00         2\n",
      "         132       1.00      1.00      1.00         2\n",
      "         133       1.00      1.00      1.00         2\n",
      "         134       1.00      1.00      1.00         2\n",
      "         135       1.00      1.00      1.00         2\n",
      "         136       1.00      1.00      1.00         2\n",
      "         137       1.00      1.00      1.00         2\n",
      "         138       1.00      1.00      1.00         2\n",
      "         139       1.00      1.00      1.00         2\n",
      "         140       1.00      1.00      1.00         2\n",
      "         141       1.00      1.00      1.00         2\n",
      "\n",
      "    accuracy                           0.92       284\n",
      "   macro avg       0.94      0.92      0.92       284\n",
      "weighted avg       0.94      0.92      0.92       284\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\talha\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "GNB.fit(strain_X, strain_Y)\n",
    "result = GNB.predict(stest_X)\n",
    "evaluation(stest_Y, result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
